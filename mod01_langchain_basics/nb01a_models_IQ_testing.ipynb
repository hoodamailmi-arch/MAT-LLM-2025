{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682c4c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe84af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU \"langchain[groq]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e461e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f93f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Not much! I'm just an AI, I don't have personal experiences or emotions, but I'm here to help you with any questions or topics you'd like to discuss. How about you? How's your day going so far?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 13, 'total_tokens': 62, 'completion_time': 0.034935283, 'prompt_time': 0.00500913, 'queue_time': 0.0591063, 'total_time': 0.039944413}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4c291295-675b-4e8f-8163-4982a2a27a5f-0', usage_metadata={'input_tokens': 13, 'output_tokens': 49, 'total_tokens': 62})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple model call\n",
    "\n",
    "model.invoke(\"Whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd459f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42416cbd",
   "metadata": {},
   "source": [
    "[Exercise] Play along. Give bigger and bigger prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da2738af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm a large language model trained on a massive dataset of text from the internet, which allows me to generate human-like responses to a wide range of topics and questions.\\n\\nI can assist with various tasks, such as:\\n\\n1. Answering questions: I can provide information on various topics, from science and history to entertainment and culture.\\n2. Generating text: I can create text based on a prompt or topic, and even help with writing tasks like proofreading and editing.\\n3. Translation: I can translate text from one language to another, including popular languages like Spanish, French, Chinese, and many more.\\n4. Summarization: I can summarize long pieces of text into shorter, more digestible versions, highlighting the main points and key information.\\n5. Conversation: I can engage in natural-sounding conversations, using context and understanding to respond to questions and statements.\\n\\nI'm constantly learning and improving, so please bear with me if I make any mistakes. I'm here to help and provide assistance to the best of my abilities!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 237, 'prompt_tokens': 14, 'total_tokens': 251, 'completion_time': 0.169426148, 'prompt_time': 0.002046548, 'queue_time': 0.044452872, 'total_time': 0.171472696}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--95fc76f5-3eec-4b85-97a9-dda4dcd8e96b-0', usage_metadata={'input_tokens': 14, 'output_tokens': 237, 'total_tokens': 251})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f4578d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a Python code that multiplies two matrices of arbitrary order, but checks if the dimensions are compatible for multiplication before performing the operation. If the dimensions are not compatible, it throws a ValueError.\\n\\n```python\\nimport numpy as np\\n\\ndef multiply_matrices(A, B):\\n    # Check if the matrices can be multiplied\\n    if A.shape[1] != B.shape[0]:\\n        raise ValueError(\"Matrices are not compatible for multiplication\")\\n\\n    # Calculate the product of the matrices\\n    C = np.dot(A, B)\\n\\n    return C\\n\\n# Example usage:\\nA = np.array([[1, 2], [3, 4]])\\nB = np.array([[5, 6], [7, 8]])\\n\\nC = multiply_matrices(A, B)\\nprint(C)\\n```\\n\\nIn this code, we use the NumPy library to create and manipulate matrices. The `shape` attribute of a NumPy array returns a tuple representing the dimensions of the array. In the `multiply_matrices` function, we check if the number of columns in the first matrix (`A.shape[1]`) is equal to the number of rows in the second matrix (`B.shape[0]`) before attempting to multiply them. If the dimensions are not compatible, we raise a ValueError. If the dimensions are compatible, we use the `np.dot` function to calculate the product of the matrices and return the result.\\n\\nYou can modify the example usage to test the function with different input matrices.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 301, 'prompt_tokens': 43, 'total_tokens': 344, 'completion_time': 0.214548059, 'prompt_time': 0.007706967, 'queue_time': 0.049364802, 'total_time': 0.222255026}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f15e5052-c1af-4543-bf9e-6770661c301b-0', usage_metadata={'input_tokens': 43, 'output_tokens': 301, 'total_tokens': 344})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"\"\" Write a python code which can multiply two matrix of arbitrary but compatible order. The code should throw exception if the matrix dimentsions are not compatible for multiplication             \n",
    "             \"\"\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c8c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Python code that multiplies two matrices of arbitrary order, but checks if the dimensions are compatible for multiplication before performing the operation. If the dimensions are not compatible, it throws a ValueError.\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def multiply_matrices(A, B):\n",
      "    # Check if the matrices can be multiplied\n",
      "    if A.shape[1] != B.shape[0]:\n",
      "        raise ValueError(\"Matrices are not compatible for multiplication\")\n",
      "\n",
      "    # Calculate the product of the matrices\n",
      "    C = np.dot(A, B)\n",
      "\n",
      "    return C\n",
      "\n",
      "# Example usage:\n",
      "A = np.array([[1, 2], [3, 4]])\n",
      "B = np.array([[5, 6], [7, 8]])\n",
      "\n",
      "C = multiply_matrices(A, B)\n",
      "print(C)\n",
      "```\n",
      "\n",
      "In this code, we use the NumPy library to create and manipulate matrices. The `shape` attribute of a NumPy array returns a tuple representing the dimensions of the array. In the `multiply_matrices` function, we check if the number of columns in the first matrix (`A.shape[1]`) is equal to the number of rows in the second matrix (`B.shape[0]`) before attempting to multiply them. If the dimensions are not compatible, we raise a ValueError. If the dimensions are compatible, we use the `np.dot` function to calculate the product of the matrices and return the result.\n",
      "\n",
      "You can modify the example usage to test the function with different input matrices.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b215da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='নমস্কার! (Namoskar!)', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 25, 'total_tokens': 41, 'completion_time': 0.011404479, 'prompt_time': 0.009291072, 'queue_time': 0.116232177, 'total_time': 0.020695551}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3574a480-5fbb-42e3-8ed4-a3dc8c274438-0', usage_metadata={'input_tokens': 25, 'output_tokens': 16, 'total_tokens': 41})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoking to build a converstation style call\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Bengali\"), # try punjabi, or any other Indian language\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9efe5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"Generate python code for given tasks\"),\n",
    "    HumanMessage(\"Find max of given n numbers\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17d70ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple Python code that takes `n` numbers as input and returns the maximum number:\n",
      "```\n",
      "def find_max(n):\n",
      "    max_num = float('-inf')  # initialize max_num to negative infinity\n",
      "    for i in range(n):\n",
      "        num = int(input(f\"Enter number {i+1}: \"))\n",
      "        if num > max_num:\n",
      "            max_num = num\n",
      "    return max_num\n",
      "\n",
      "n = int(input(\"Enter the number of numbers: \"))\n",
      "print(\"The maximum number is:\", find_max(n))\n",
      "```\n",
      "Here's how the code works:\n",
      "\n",
      "1. We initialize `max_num` to negative infinity (`float('-inf')`) to ensure that any number input by the user will be greater than `max_num`.\n",
      "2. We loop `n` times, asking the user to input a number each time.\n",
      "3. For each input number, we check if it is greater than `max_num`. If it is, we update `max_num` to be that number.\n",
      "4. Finally, we return `max_num`, which is the maximum number input by the user.\n",
      "\n",
      "You can run this code and input numbers to find the maximum number. For example, if you input `3` and then enter the numbers `10`, `20`, and `5`, the output will be `20`, which is the maximum number.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36098aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple Python code that takes `n` numbers as input and returns the maximum number:\n",
      "```\n",
      "def find_max(n):\n",
      "    max_num = float('-inf')  # initialize max_num to negative infinity\n",
      "    for i in range(n):\n",
      "        num = int(input(f\"Enter number {i+1}: \"))\n",
      "        if num > max_num:\n",
      "            max_num = num\n",
      "    return max_num\n",
      "\n",
      "n = int(input(\"Enter the number of numbers: \"))\n",
      "print(\"The maximum number is:\", find_max(n))\n",
      "```\n",
      "Here's how the code works:\n",
      "\n",
      "1. We initialize `max_num` to negative infinity (`float('-inf')`) to ensure that any number input by the user will be greater than `max_num`.\n",
      "2. We loop `n` times, asking the user to input a number each time.\n",
      "3. For each input number, we check if it is greater than `max_num`. If it is, we update `max_num` to be that number.\n",
      "4. Finally, we return `max_num`, which is the maximum number input by the user.\n",
      "\n",
      "You can run this code and input numbers to find the maximum number. For example, if you input `3` and then enter the numbers `10`, `20`, and `5`, the output will be `20`, which is the maximum number.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39caa649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content='Hello' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' It' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=\"'s\" additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' nice' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' to' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' meet' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content='.' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' Is' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' there' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' something' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' I' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' can' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' help' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' with' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' or' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' would' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' you' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' like' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' to' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content=' chat' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content='?' additional_kwargs={} response_metadata={} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_4b5fbf0ced', 'service_tier': 'on_demand'} id='run--024cc5b0-9c34-4b51-b41b-9b1b39adfbe5' usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37}\n"
     ]
    }
   ],
   "source": [
    "# streaming example\n",
    "import time\n",
    "\n",
    "for token in model.stream(\"hello\"):\n",
    "    time.sleep(0.1)\n",
    "    #print(token.content, end=\"|\")\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f083693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseChatModel.stream at 0x0000018FDA02EF80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stream(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class discussion point: What is an LLM as a program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ca534bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model1 = init_chat_model(\"meta-llama/llama-4-maverick-17b-128e-instruct\", model_provider=\"groq\")\n",
    "\n",
    "# exercise: Replace this model with some other model on the groq website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78c8fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408f739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mat496-monsoon2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
