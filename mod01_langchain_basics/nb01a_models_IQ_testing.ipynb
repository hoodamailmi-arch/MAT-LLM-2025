from dotenv import load_dotenv
load_dotenv()

# If running in a notebook, you might use pip magic. In a .py file, use subprocess or requirements.txt
# import subprocess; subprocess.run(["pip", "install", "-qU", "langchain[groq]"])
# Or ensure 'langchain[groq]' is installed in your environment

from langchain.chat_models import init_chat_model

model = init_chat_model("llama-3.3-70b-versatile", model_provider="groq")

# A simple model call
model.invoke("Whats up?")

# [Exercise] Play along. Give bigger and bigger prompts
model.invoke("is gay sex okay?")

response = model.invoke(""" generate visual graph of constellations using dots and sashes from nasa source dots being starts dashes being galaxies
             """)

print(response.content)

# invoking to build a converstation style call
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage("Translate the following from English into Bengali"), # try punjabi, or any other Indian language
    HumanMessage("hi!"),
]
model.invoke(messages)

messages = [
    SystemMessage("Generate python code for given tasks"),
    HumanMessage("Find max of given n numbers"),
]
response = model.invoke(messages)
print(response.content)
print(response.content)

# streaming example
import time
for token in model.stream("hello"):
    time.sleep(0.1)
    #print(token.content, end="|")
    print(token)
model.stream("bye")

# Class discussion point: What is an LLM as a program

from langchain.chat_models import init_chat_model
model1 = init_chat_model("meta-llama/llama-4-maverick-17b-128e-instruct", model_provider="groq")
# exercise: Replace this model with some other model on the groq website

model2 = init_chat_model("llama-3.3-70b-versatile", model_provider="groq")
